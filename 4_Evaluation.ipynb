{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation (AI-as-a-Judge)\n",
    "\n",
    "마지막 단계에서는 우리가 만든 AI 수의사들의 실력을 평가합니다.\n",
    "사람이 직접 채점하는 대신, 또 다른 AI(LLM)에게 채점 기준표를 주고 평가를 맡기는 **Model-based Evaluation** 방식을 실습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI 클라이언트 설정 (API Key 하드코딩)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정답 및 결과 로드\n",
    "5개 케이스 각각에 대한 정답과 에이전트들의 결과를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 ground truth cases.\n"
     ]
    }
   ],
   "source": [
    "# 정답 데이터 로드\n",
    "df_truth = pd.read_csv(\"./data/processed_sample.csv\")\n",
    "print(f\"Loaded {len(df_truth)} ground truth cases.\")\n",
    "\n",
    "# 결과 로드 함수\n",
    "def load_result(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "# 모든 결과를 리스트로 로드\n",
    "single_results = []\n",
    "multi_results = []\n",
    "\n",
    "for i in range(len(df_truth)):\n",
    "    single_results.append(load_result(f\"./data/single_result_{i}.md\"))\n",
    "    multi_results.append(load_result(f\"./data/multi_result_{i}.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 평가 함수 작성\n",
    "채점관 AI에게 줄 '채점 기준표'를 작성합니다.\n",
    "정확성(Accuracy), 구체성(Completeness), 공감능력(Empathy) 세 가지 항목으로 평가하도록 지시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(answer, truth, problem_desc, criteria):\n",
    "    prompt = f\"\"\"\n",
    "    당신은 수의학 전문 평가자입니다.\n",
    "    실제 수의사의 답변(Ground Truth)과 문제 상황(Problem Description)을 바탕으로 학생(AI)의 답변을 평가하세요.\n",
    "    \n",
    "    문제 상황 (Problem Description):\n",
    "    {problem_desc}\n",
    "    \n",
    "    실제 수의사 정답 (Ground Truth):\n",
    "    {truth}\n",
    "    \n",
    "    학생 AI 답변:\n",
    "    {answer}\n",
    "    \n",
    "    평가 기준:\n",
    "    {criteria}\n",
    "    \n",
    "    평가 결과는 반드시 아래와 같은 정확한 구조의 JSON 객체로 제공해야 합니다:\n",
    "    {{\n",
    "      \"Accuracy\": {{\"score\": 0, \"reasoning\": \"text\"}},\n",
    "      \"Completeness\": {{\"score\": 0, \"reasoning\": \"text\"}},\n",
    "      \"Empathy\": {{\"score\": 0, \"reasoning\": \"text\"}}\n",
    "    }}\n",
    "    (0을 0-10 사이의 점수로, text를 한국어로 작성된 점수 부여 사유로 교체하세요).\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 평가 실행 (반복)\n",
    "모든 케이스에 대해 점수를 매기고 평균을 냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Case #1...\n",
      "Evaluating Case #2...\n",
      "Evaluating Case #3...\n",
      "Evaluating Case #4...\n",
      "Evaluating Case #5...\n",
      "\n",
      "--- Average Single Agent Scores ---\n",
      "{'Accuracy': 9.6, 'Completeness': 9.6, 'Empathy': 9.6}\n",
      "\n",
      "--- Average Multi Agent Scores ---\n",
      "{'Accuracy': 5.8, 'Completeness': 5.6, 'Empathy': 6.0}\n",
      "\n",
      "--- Average Vet (Ground Truth) Scores ---\n",
      "{'Accuracy': 9.4, 'Completeness': 8.6, 'Empathy': 9.4}\n"
     ]
    }
   ],
   "source": [
    "criteria = \"\"\"\n",
    "1. Accuracy (정확성): 의학적 조언이 올바르고 안전한가?\n",
    "2. Completeness (완전성): 긴급도, 원인, 가정 내 대처법을 모두 다루었는가?\n",
    "3. Empathy (공감능력): 걱정하는 보호자에게 적절하고 친절한 어조인가?\n",
    "\"\"\"\n",
    "\n",
    "all_single_scores = []\n",
    "all_multi_scores = []\n",
    "all_vet_scores = []\n",
    "\n",
    "for i in range(len(df_truth)):\n",
    "    print(f\"Evaluating Case #{i+1}...\")\n",
    "    ground_truth = df_truth.iloc[i]['answer']\n",
    "    problem_desc = df_truth.iloc[i]['formatted_prompt']\n",
    "    \n",
    "    s_score = evaluate_answer(single_results[i], ground_truth, problem_desc, criteria)\n",
    "    m_score = evaluate_answer(multi_results[i], ground_truth, problem_desc, criteria)\n",
    "    v_score = evaluate_answer(ground_truth, ground_truth, problem_desc, criteria)\n",
    "    \n",
    "    if s_score: all_single_scores.append(s_score)\n",
    "    if m_score: all_multi_scores.append(m_score)\n",
    "    if v_score: all_vet_scores.append(v_score)\n",
    "\n",
    "# 결과를 DataFrame으로 정리하여 평균 계산\n",
    "def get_avg_scores(scores_list):\n",
    "    if not scores_list: return {}\n",
    "    avg = {'Accuracy': 0, 'Completeness': 0, 'Empathy': 0}\n",
    "    for s in scores_list:\n",
    "        avg['Accuracy'] += s['Accuracy']['score']\n",
    "        avg['Completeness'] += s['Completeness']['score']\n",
    "        avg['Empathy'] += s['Empathy']['score']\n",
    "    for k in avg:\n",
    "        avg[k] /= len(scores_list)\n",
    "    return avg\n",
    "\n",
    "avg_single = get_avg_scores(all_single_scores)\n",
    "avg_multi = get_avg_scores(all_multi_scores)\n",
    "avg_vet = get_avg_scores(all_vet_scores)\n",
    "\n",
    "print(\"\\n--- Average Single Agent Scores ---\")\n",
    "print(avg_single)\n",
    "print(\"\\n--- Average Multi Agent Scores ---\")\n",
    "print(avg_multi)\n",
    "print(\"\\n--- Average Vet (Ground Truth) Scores ---\")\n",
    "print(avg_vet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if avg_single and avg_multi and avg_vet:\n",
    "    df_scores = pd.DataFrame({\n",
    "        'Metric': list(avg_single.keys()),\n",
    "        'Single Agent': list(avg_single.values()),\n",
    "        'Multi Agent': list(avg_multi.values()),\n",
    "        'Vet (Ground Truth)': list(avg_vet.values())\n",
    "    })\n",
    "\n",
    "    df_scores.set_index('Metric').plot(kind='bar', ylim=(0, 10), rot=0)\n",
    "    plt.title('Performance Comparison (Avg of 5 Cases)')\n",
    "    plt.ylabel('Score (0-10)')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}